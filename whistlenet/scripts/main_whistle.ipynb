{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whistle Detection with Continuous Kernel Convolutional Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "import os\n",
    "from whistlenet.core.utils import project_root\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "config_path = os.path.join(project_root(), \"config\",\"whistle_config.yaml\")\n",
    "config: Config = load_config(config_path)\n",
    "torch_config: TorchConfig = config.torch\n",
    "dataset_config: DatasetConfig = config.dataset\n",
    "trainer_config: TrainerConfig = config.trainer\n",
    "baseline_config: BaselineConfig = config.baseline\n",
    "whistlenet_config: WhistlenetConfig = config.whistlenet\n",
    "\n",
    "from config.enums import Optimizer\n",
    "\n",
    "print(list(Optimizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "torch.manual_seed(torch_config.seed)\n",
    "np.random.seed(torch_config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whistlenet.data import WhistleDataset\n",
    "from whistlenet.core.utils import plot, NUM_FREQS\n",
    "from whistlenet.core.utils.audio import SampleType\n",
    "\n",
    "dataset = WhistleDataset(dataset_config)\n",
    "dataset.summarize()\n",
    "reshaped = dataset.train_data.reshape((dataset.train_data.data.shape[0], NUM_FREQS))\n",
    "plot(reshaped, dataset.train_data.labels, SampleType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whistlenet.core.utils import Audio, project_root\n",
    "projroot = project_root()\n",
    "audio = Audio(name=\"KronosTest_RC22\", datapath=f'{projroot}/data/whistle/raw/train_cut', labelpath=f'{projroot}/data/whistle/labels/train_cut')\n",
    "audio.freq_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whistlenet.models import WhistleNet, Baseline\n",
    "from whistlenet.core import LightningTrainer\n",
    "\n",
    "model = WhistleNet(in_channels=1, out_channels=1, config=whistlenet_config)\n",
    "trainer = LightningTrainer(trainer_config)\n",
    "trainer.fit(model,dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference on test audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import lightning as L\n",
    "\n",
    "audiolabels = audio.get_labels()\n",
    "# checkpoint_path = os.path.join(trainer_config.ckpt_path, \"epoch=7-step=14376.ckpt\")\n",
    "# model = WhistleNet.load_from_checkpoint(checkpoint_path, in_channels = 1, out_channels = 1, config=whistlenet_config)\n",
    "\n",
    "def normalize(data: torch.Tensor, epsilon: float = 1e-6) -> torch.Tensor:\n",
    "    min = data.min()\n",
    "    max = data.max()\n",
    "    data = (data - min) / (\n",
    "        max - min + epsilon\n",
    "    )  # Adding epsilon to avoid division by zero\n",
    "    return data\n",
    "\n",
    "def classify(window):\n",
    "    with torch.inference_mode():\n",
    "        start = time.time()\n",
    "        confidence = model(window).item()\n",
    "        print(f\"Confidence: {confidence:.4f}\")\n",
    "        prediction = 1 if confidence > 0.5 else 0\n",
    "        elapsed = time.time() - start\n",
    "        print(f\"Prediction: {prediction}, elapsed: {elapsed:.4f}\")\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(audio.frames):\n",
    "    window = torch.from_numpy(audio.S[0,:,i].reshape(1,1,NUM_FREQS))\n",
    "    window = normalize(window)\n",
    "    print(f\"frame {i}, time {audio.frame2time(i):.2f}\")\n",
    "    print(f\"label: {audiolabels[0,i]}\")\n",
    "    classify(window)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = f\"{projroot}/models/{model.name}/whistle.onnx\"\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    model.example_input[0],\n",
    "    f = file,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
