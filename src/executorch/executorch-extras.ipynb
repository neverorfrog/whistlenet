{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import traceback as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Expected an nn.Module instance.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m y\n\u001b[1;32m      6\u001b[0m example_args \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m----> 8\u001b[0m pre_autograd_aten_dialect \u001b[38;5;241m=\u001b[39m \u001b[43mcapture_pre_autograd_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m aten_dialect: ExportedProgram \u001b[38;5;241m=\u001b[39m export(pre_autograd_aten_dialect, example_args)\n",
      "File \u001b[0;32m~/.miniconda3/envs/executorch/lib/python3.10/site-packages/torch/_export/__init__.py:135\u001b[0m, in \u001b[0;36mcapture_pre_autograd_graph\u001b[0;34m(f, args, kwargs, dynamic_shapes)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexport\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_shapes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _process_dynamic_shapes\n\u001b[1;32m    133\u001b[0m log_export_usage(event\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexport.private_api\u001b[39m\u001b[38;5;124m\"\u001b[39m, flags\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcapture_pre_autograd_graph\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected an nn.Module instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mAssertionError\u001b[0m: Expected an nn.Module instance."
     ]
    }
   ],
   "source": [
    "from torch._export import capture_pre_autograd_graph\n",
    "from torch.export import export, ExportedProgram\n",
    "\n",
    "def f(x, y):\n",
    "    return x + y\n",
    "example_args = (torch.randn(3, 3), torch.randn(3, 3))\n",
    "\n",
    "pre_autograd_aten_dialect = capture_pre_autograd_graph(f, example_args)\n",
    "aten_dialect: ExportedProgram = export(pre_autograd_aten_dialect, example_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some details\n",
    "\n",
    "Different dimensions than the ones expected from our model (obviously) break things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works correctly\n",
    "print(aten_dialect(torch.ones(3, 3), torch.ones(3, 3)))\n",
    "\n",
    "# Errors\n",
    "try:\n",
    "    print(aten_dialect(torch.ones(3, 2), torch.ones(3, 2)))\n",
    "except Exception as e:\n",
    "    tb.print_exc(limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there can be some dynamism. We can put some constraints on the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.export import dynamic_dim\n",
    "constraints = [\n",
    "    # Input 0, dimension 1 is dynamic\n",
    "    dynamic_dim(example_args[0], 1),\n",
    "    # Input 0, dimension 1 must be greater than or equal to 1\n",
    "    1 <= dynamic_dim(example_args[0], 1),\n",
    "    # Input 0, dimension 1 must be less than or equal to 10\n",
    "    dynamic_dim(example_args[0], 1) <= 10,\n",
    "    # Input 1, dimension 1 is equal to input 0, dimension 1\n",
    "    dynamic_dim(example_args[1], 1) == dynamic_dim(example_args[0], 1),\n",
    "]\n",
    "pre_autograd_aten_dialect = capture_pre_autograd_graph(\n",
    "    f, example_args, constraints=constraints\n",
    ")\n",
    "aten_dialect: ExportedProgram = export(f, example_args, constraints=constraints)\n",
    "\n",
    "# Works correctly\n",
    "print(aten_dialect(torch.ones(3, 3), torch.ones(3, 3)))\n",
    "print(aten_dialect(torch.ones(3, 2), torch.ones(3, 2)))\n",
    "\n",
    "# Errors because it violates our constraint that input 0, dim 1 <= 10\n",
    "try:\n",
    "    print(aten_dialect(torch.ones(3, 15), torch.ones(3, 15)))\n",
    "except Exception:\n",
    "    tb.print_exc(limit=1)\n",
    "    \n",
    "# Errors because it violates our constraint that input 0, dim 1 == input 1, dim 1\n",
    "try:\n",
    "    print(aten_dialect(torch.ones(3, 3), torch.ones(3, 2)))\n",
    "except Exception:\n",
    "    tb.print_exc(limit=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "executorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
