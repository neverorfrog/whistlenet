{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from utils import project_root\n",
    "projroot = project_root()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL LOADED!\n",
      "tensor([[3.3871, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from models import ToyModel, WhistleNet, CCNN\n",
    "import yaml\n",
    "from omegaconf import OmegaConf\n",
    "from matplotlib import pyplot as plt\n",
    "from utils import num_params\n",
    "\n",
    "# Load the configuration\n",
    "file_path = \"ccnn/config.yaml\"\n",
    "with open(file_path, 'r') as file:\n",
    "    config = OmegaConf.create(yaml.safe_load(file))\n",
    "\n",
    "name = \"whistle/boh\"\n",
    "torch_model = WhistleNet(name)\n",
    "torch_model.load(name)\n",
    "torch_input = torch_model.example_input[0]\n",
    "name = \"boh\"\n",
    "torch_out = torch_model(torch_input)\n",
    "print(torch_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported graph: graph(%input : Float(1, 1, 513, strides=[513, 513, 1], requires_grad=0, device=cpu),\n",
      "      %conv.15.weight : Float(64, 128, 5, strides=[640, 5, 1], requires_grad=1, device=cpu),\n",
      "      %conv.15.bias : Float(64, strides=[1], requires_grad=1, device=cpu),\n",
      "      %fc.0.weight : Float(2, 3520, strides=[3520, 1], requires_grad=1, device=cpu),\n",
      "      %fc.0.bias : Float(2, strides=[1], requires_grad=1, device=cpu),\n",
      "      %onnx::Conv_115 : Float(32, 1, 5, strides=[5, 5, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_116 : Float(32, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_118 : Float(64, 32, 5, strides=[160, 5, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_119 : Float(64, strides=[1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_121 : Float(128, 64, 5, strides=[320, 5, 1], requires_grad=0, device=cpu),\n",
      "      %onnx::Conv_122 : Float(128, strides=[1], requires_grad=0, device=cpu)):\n",
      "  %onnx::Conv_26 : Float(1, 1, 513, strides=[513, 513, 1], requires_grad=0, device=cpu) = onnx::Cast[to=1](%input) # /home/neverorfrog/code/whistle/whistlenet/tests/../src/models/whistlenet.py:107:0\n",
      "  %conv/conv.0/Conv_output_0 : Float(1, 32, 255, strides=[8160, 255, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[5], pads=[0, 0], strides=[2], onnx_name=\"conv/conv.0/Conv\"](%onnx::Conv_26, %onnx::Conv_115, %onnx::Conv_116), scope: torch.nn.modules.container.Sequential::conv/torch.nn.modules.conv.Conv1d::conv.0 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/modules/conv.py:306:0\n",
      "  %conv/conv.2/LeakyRelu_output_0 : Float(1, 32, 255, strides=[8160, 255, 1], requires_grad=1, device=cpu) = onnx::LeakyRelu[alpha=0.10000000000000001, onnx_name=\"conv/conv.2/LeakyRelu\"](%conv/conv.0/Conv_output_0), scope: torch.nn.modules.container.Sequential::conv/torch.nn.modules.activation.LeakyReLU::conv.2 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:1675:0\n",
      "  %conv/conv.3/Constant_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name=\"conv/conv.3/Constant\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Constant_1_output_0 : Long(2, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 0  0 [ CPULongType{2} ], onnx_name=\"conv/conv.3/Constant_1\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/ConstantOfShape_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={0}, onnx_name=\"conv/conv.3/ConstantOfShape\"](%conv/conv.3/Constant_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Concat_output_0 : Long(6, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"conv/conv.3/Concat\"](%conv/conv.3/Constant_1_output_0, %conv/conv.3/ConstantOfShape_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Constant_2_output_0 : Long(2, strides=[1], device=cpu) = onnx::Constant[value=-1  2 [ CPULongType{2} ], onnx_name=\"conv/conv.3/Constant_2\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Reshape_output_0 : Long(3, 2, strides=[2, 1], device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"conv/conv.3/Reshape\"](%conv/conv.3/Concat_output_0, %conv/conv.3/Constant_2_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Constant_3_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name=\"conv/conv.3/Constant_3\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Constant_4_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}, onnx_name=\"conv/conv.3/Constant_4\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Constant_5_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-9223372036854775807}, onnx_name=\"conv/conv.3/Constant_5\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Constant_6_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}, onnx_name=\"conv/conv.3/Constant_6\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Slice_output_0 : Long(3, 2, strides=[2, 1], device=cpu) = onnx::Slice[onnx_name=\"conv/conv.3/Slice\"](%conv/conv.3/Reshape_output_0, %conv/conv.3/Constant_4_output_0, %conv/conv.3/Constant_5_output_0, %conv/conv.3/Constant_3_output_0, %conv/conv.3/Constant_6_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Transpose_output_0 : Long(2, 3, strides=[3, 1], device=cpu) = onnx::Transpose[perm=[1, 0], onnx_name=\"conv/conv.3/Transpose\"](%conv/conv.3/Slice_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Constant_7_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}, onnx_name=\"conv/conv.3/Constant_7\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Reshape_1_output_0 : Long(6, strides=[1], device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"conv/conv.3/Reshape_1\"](%conv/conv.3/Transpose_output_0, %conv/conv.3/Constant_7_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Cast_output_0 : Long(6, strides=[1], device=cpu) = onnx::Cast[to=7, onnx_name=\"conv/conv.3/Cast\"](%conv/conv.3/Reshape_1_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Constant_8_output_0 : Float(device=cpu) = onnx::Constant[value={0}, onnx_name=\"conv/conv.3/Constant_8\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/Pad_output_0 : Float(*, *, *, strides=[8160, 255, 1], requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", onnx_name=\"conv/conv.3/Pad\"](%conv/conv.2/LeakyRelu_output_0, %conv/conv.3/Cast_output_0, %conv/conv.3/Constant_8_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.3/MaxPool_output_0 : Float(*, *, *, strides=[8128, 254, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1], kernel_shape=[2], pads=[0, 0], strides=[1], onnx_name=\"conv/conv.3/MaxPool\"](%conv/conv.3/Pad_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.3 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:710:0\n",
      "  %conv/conv.5/Conv_output_0 : Float(*, 64, *, strides=[8000, 125, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[5], pads=[0, 0], strides=[2], onnx_name=\"conv/conv.5/Conv\"](%conv/conv.3/MaxPool_output_0, %onnx::Conv_118, %onnx::Conv_119), scope: torch.nn.modules.container.Sequential::conv/torch.nn.modules.conv.Conv1d::conv.5 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/modules/conv.py:306:0\n",
      "  %conv/conv.7/LeakyRelu_output_0 : Float(*, 64, *, strides=[8000, 125, 1], requires_grad=1, device=cpu) = onnx::LeakyRelu[alpha=0.10000000000000001, onnx_name=\"conv/conv.7/LeakyRelu\"](%conv/conv.5/Conv_output_0), scope: torch.nn.modules.container.Sequential::conv/torch.nn.modules.activation.LeakyReLU::conv.7 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:1675:0\n",
      "  %conv/conv.8/Constant_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name=\"conv/conv.8/Constant\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Constant_1_output_0 : Long(2, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 0  0 [ CPULongType{2} ], onnx_name=\"conv/conv.8/Constant_1\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/ConstantOfShape_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={0}, onnx_name=\"conv/conv.8/ConstantOfShape\"](%conv/conv.8/Constant_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Concat_output_0 : Long(6, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"conv/conv.8/Concat\"](%conv/conv.8/Constant_1_output_0, %conv/conv.8/ConstantOfShape_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Constant_2_output_0 : Long(2, strides=[1], device=cpu) = onnx::Constant[value=-1  2 [ CPULongType{2} ], onnx_name=\"conv/conv.8/Constant_2\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Reshape_output_0 : Long(3, 2, strides=[2, 1], device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"conv/conv.8/Reshape\"](%conv/conv.8/Concat_output_0, %conv/conv.8/Constant_2_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Constant_3_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name=\"conv/conv.8/Constant_3\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Constant_4_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}, onnx_name=\"conv/conv.8/Constant_4\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Constant_5_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-9223372036854775807}, onnx_name=\"conv/conv.8/Constant_5\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Constant_6_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}, onnx_name=\"conv/conv.8/Constant_6\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Slice_output_0 : Long(3, 2, strides=[2, 1], device=cpu) = onnx::Slice[onnx_name=\"conv/conv.8/Slice\"](%conv/conv.8/Reshape_output_0, %conv/conv.8/Constant_4_output_0, %conv/conv.8/Constant_5_output_0, %conv/conv.8/Constant_3_output_0, %conv/conv.8/Constant_6_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Transpose_output_0 : Long(2, 3, strides=[3, 1], device=cpu) = onnx::Transpose[perm=[1, 0], onnx_name=\"conv/conv.8/Transpose\"](%conv/conv.8/Slice_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Constant_7_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}, onnx_name=\"conv/conv.8/Constant_7\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Reshape_1_output_0 : Long(6, strides=[1], device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"conv/conv.8/Reshape_1\"](%conv/conv.8/Transpose_output_0, %conv/conv.8/Constant_7_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Cast_output_0 : Long(6, strides=[1], device=cpu) = onnx::Cast[to=7, onnx_name=\"conv/conv.8/Cast\"](%conv/conv.8/Reshape_1_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Constant_8_output_0 : Float(device=cpu) = onnx::Constant[value={0}, onnx_name=\"conv/conv.8/Constant_8\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/Pad_output_0 : Float(*, *, *, strides=[8000, 125, 1], requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", onnx_name=\"conv/conv.8/Pad\"](%conv/conv.7/LeakyRelu_output_0, %conv/conv.8/Cast_output_0, %conv/conv.8/Constant_8_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.8/MaxPool_output_0 : Float(*, *, *, strides=[7936, 124, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1], kernel_shape=[2], pads=[0, 0], strides=[1], onnx_name=\"conv/conv.8/MaxPool\"](%conv/conv.8/Pad_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.8 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:710:0\n",
      "  %conv/conv.10/Conv_output_0 : Float(*, 128, *, strides=[7680, 60, 1], requires_grad=1, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[5], pads=[0, 0], strides=[2], onnx_name=\"conv/conv.10/Conv\"](%conv/conv.8/MaxPool_output_0, %onnx::Conv_121, %onnx::Conv_122), scope: torch.nn.modules.container.Sequential::conv/torch.nn.modules.conv.Conv1d::conv.10 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/modules/conv.py:306:0\n",
      "  %conv/conv.12/LeakyRelu_output_0 : Float(*, 128, *, strides=[7680, 60, 1], requires_grad=1, device=cpu) = onnx::LeakyRelu[alpha=0.10000000000000001, onnx_name=\"conv/conv.12/LeakyRelu\"](%conv/conv.10/Conv_output_0), scope: torch.nn.modules.container.Sequential::conv/torch.nn.modules.activation.LeakyReLU::conv.12 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:1675:0\n",
      "  %conv/conv.13/Constant_output_0 : Long(1, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value={4}, onnx_name=\"conv/conv.13/Constant\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Constant_1_output_0 : Long(2, strides=[1], requires_grad=0, device=cpu) = onnx::Constant[value= 0  0 [ CPULongType{2} ], onnx_name=\"conv/conv.13/Constant_1\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/ConstantOfShape_output_0 : Long(4, strides=[1], device=cpu) = onnx::ConstantOfShape[value={0}, onnx_name=\"conv/conv.13/ConstantOfShape\"](%conv/conv.13/Constant_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Concat_output_0 : Long(6, strides=[1], device=cpu) = onnx::Concat[axis=0, onnx_name=\"conv/conv.13/Concat\"](%conv/conv.13/Constant_1_output_0, %conv/conv.13/ConstantOfShape_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Constant_2_output_0 : Long(2, strides=[1], device=cpu) = onnx::Constant[value=-1  2 [ CPULongType{2} ], onnx_name=\"conv/conv.13/Constant_2\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Reshape_output_0 : Long(3, 2, strides=[2, 1], device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"conv/conv.13/Reshape\"](%conv/conv.13/Concat_output_0, %conv/conv.13/Constant_2_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Constant_3_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={0}, onnx_name=\"conv/conv.13/Constant_3\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Constant_4_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}, onnx_name=\"conv/conv.13/Constant_4\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Constant_5_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-9223372036854775807}, onnx_name=\"conv/conv.13/Constant_5\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Constant_6_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}, onnx_name=\"conv/conv.13/Constant_6\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Slice_output_0 : Long(3, 2, strides=[2, 1], device=cpu) = onnx::Slice[onnx_name=\"conv/conv.13/Slice\"](%conv/conv.13/Reshape_output_0, %conv/conv.13/Constant_4_output_0, %conv/conv.13/Constant_5_output_0, %conv/conv.13/Constant_3_output_0, %conv/conv.13/Constant_6_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Transpose_output_0 : Long(2, 3, strides=[3, 1], device=cpu) = onnx::Transpose[perm=[1, 0], onnx_name=\"conv/conv.13/Transpose\"](%conv/conv.13/Slice_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Constant_7_output_0 : Long(1, strides=[1], device=cpu) = onnx::Constant[value={-1}, onnx_name=\"conv/conv.13/Constant_7\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Reshape_1_output_0 : Long(6, strides=[1], device=cpu) = onnx::Reshape[allowzero=0, onnx_name=\"conv/conv.13/Reshape_1\"](%conv/conv.13/Transpose_output_0, %conv/conv.13/Constant_7_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Cast_output_0 : Long(6, strides=[1], device=cpu) = onnx::Cast[to=7, onnx_name=\"conv/conv.13/Cast\"](%conv/conv.13/Reshape_1_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Constant_8_output_0 : Float(device=cpu) = onnx::Constant[value={0}, onnx_name=\"conv/conv.13/Constant_8\"](), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/Pad_output_0 : Float(*, *, *, strides=[7680, 60, 1], requires_grad=1, device=cpu) = onnx::Pad[mode=\"constant\", onnx_name=\"conv/conv.13/Pad\"](%conv/conv.12/LeakyRelu_output_0, %conv/conv.13/Cast_output_0, %conv/conv.13/Constant_8_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:4522:0\n",
      "  %conv/conv.13/MaxPool_output_0 : Float(*, *, *, strides=[7552, 59, 1], requires_grad=1, device=cpu) = onnx::MaxPool[ceil_mode=0, dilations=[1], kernel_shape=[2], pads=[0, 0], strides=[1], onnx_name=\"conv/conv.13/MaxPool\"](%conv/conv.13/Pad_output_0), scope: torch.nn.modules.container.Sequential::conv/core.convutils.MaxPool1dSame::conv.13 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:710:0\n",
      "  %conv/conv.15/Conv_output_0 : Float(*, 64, *, strides=[3520, 55, 1], requires_grad=0, device=cpu) = onnx::Conv[dilations=[1], group=1, kernel_shape=[5], pads=[0, 0], strides=[1], onnx_name=\"conv/conv.15/Conv\"](%conv/conv.13/MaxPool_output_0, %conv.15.weight, %conv.15.bias), scope: torch.nn.modules.container.Sequential::conv/torch.nn.modules.conv.Conv1d::conv.15 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/modules/conv.py:306:0\n",
      "  %conv/conv.16/Sigmoid_output_0 : Float(*, 64, *, strides=[3520, 55, 1], requires_grad=1, device=cpu) = onnx::Sigmoid[onnx_name=\"conv/conv.16/Sigmoid\"](%conv/conv.15/Conv_output_0), scope: torch.nn.modules.container.Sequential::conv/torch.nn.modules.activation.Sigmoid::conv.16 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/modules/activation.py:292:0\n",
      "  %onnx::Gemm_111 : Float(*, *, strides=[3520, 1], requires_grad=1, device=cpu) = onnx::Flatten[axis=1](%conv/conv.16/Sigmoid_output_0) # /home/neverorfrog/code/whistle/whistlenet/tests/../src/models/whistlenet.py:112:0\n",
      "  %fc/fc.0/Gemm_output_0 : Float(*, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Gemm[alpha=1., beta=1., transB=1, onnx_name=\"fc/fc.0/Gemm\"](%onnx::Gemm_111, %fc.0.weight, %fc.0.bias), scope: torch.nn.modules.container.Sequential::fc/torch.nn.modules.linear.Linear::fc.0 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/modules/linear.py:116:0\n",
      "  %output : Float(*, 2, strides=[2, 1], requires_grad=1, device=cpu) = onnx::Relu[onnx_name=\"fc/fc.1/Relu\"](%fc/fc.0/Gemm_output_0), scope: torch.nn.modules.container.Sequential::fc/torch.nn.modules.activation.ReLU::fc.1 # /home/neverorfrog/.miniconda3/envs/ccnn/lib/python3.11/site-packages/torch/nn/functional.py:1500:0\n",
      "  return (%output)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from core import ExecutorchModel\n",
    "from edge import ONNXModel\n",
    "import onnx\n",
    "import torch\n",
    "\n",
    "torch_model.eval()\n",
    "\n",
    "torch.onnx.export(\n",
    "    torch_model,\n",
    "    torch_input,\n",
    "    f\"{name}.onnx\",\n",
    "    verbose=True,\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    ")\n",
    "\n",
    "onnx_model = onnx.load(f\"{name}.onnx\")\n",
    "onnx.checker.check_model(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(f\"{name}.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "def to_numpy(tensor):\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(torch_input)}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# compare ONNX Runtime and PyTorch results\n",
    "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
    "\n",
    "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11729550361633301\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "onnx_elapsed = 0\n",
    "iterations = 10000\n",
    "onnx_elapsed = []\n",
    "\n",
    "for i in range(iterations):\n",
    "    torch_input = torch.rand(torch_model.example_input[0].shape)\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(torch_input)}\n",
    "    start = time.time()\n",
    "    ort_session.run(None, ort_inputs)\n",
    "    end = time.time()\n",
    "    onnx_elapsed.append(end - start)\n",
    "\n",
    "print(np.mean(onnx_elapsed) * 1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
